{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalize Deep Learning Models with Azure CLI 2.0\n",
    "\n",
    "In this example, we show how to deploy the trained Keras (tensorflow) model as a web service through [Azure CLI](https://azure.microsoft.com/en-us/blog/announcing-general-availability-of-vm-storage-and-network-azure-cli-2-0/). The target machine learning problem is [CIFAR-10 - Object Recognition in Images](https://www.kaggle.com/c/cifar-10).\n",
    "\n",
    "Specifically, we train a Keras network and evaluate the trained model on testing data. In Section Model Training, we show the steps to save a Keras model, deploy the model through Azure CLI, and test the deployed web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup \n",
    "\n",
    "In this example, we use Deep Learning Virtual Machine - Linux OS, Standard [NC6 (6 vcpus, 56 GB memory) machine](https://azure.microsoft.com/en-us/blog/azure-n-series-preview-availability/) as the compute resource, where we train and deploy the model. Other types of Azure Linux VM should work as well.\n",
    "\n",
    "We use following tools on this DLVM:\n",
    "    - Python 3\n",
    "    - Jupyter Notebook \n",
    "    - Azure CLI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From your local machine ssh into the VM. In VM's CLI console, clone the repo to a desired location using below command. \n",
    "       \n",
    "       $ git clone https://github.com/FrancescaLazzeri/DLDeploymentAzureCLI.git\n",
    "        \n",
    "Launch Jupyter Server manually. It matters where and how the Jupyter server is launched. In order to make the remaining tutorial smooth, please follow below steps to launch Jupyter server. In VM's CLI console, (1) switch to use `py35` conda environment, and (2) launch Jupyter server from `py35` conda environment manually. Please follow below instructions to achieve it. \n",
    "\n",
    "    1) Change the conda environment \n",
    "        $ source activate py35 \n",
    "\n",
    "    2) Start the jupyter notebook from the command line\n",
    "        $ jupyter notebook --no-browser --ip=* \n",
    "   \n",
    "    3) In the output of above command, locate `http(s)://[all ip addresses on your system]:port_number/`. An example is, `https://[all ip addresses on your system]:9999/`. If it has `token` shown in part of this address, it is very likely you need to finish steps 7) - 10). If there is no URL in the output. Instead, you see: error message `No such notebook dir: \"/dsvm/Notebooks\"`, follow this page [Access Jupyter notebooks on Azure DSVM](http://yiyujia.blogspot.com/2018/05/access-jupyter-notebooks-on-azure-dsvm.html) to set c.NotebookApp.notebook_dir to be your home directory in `jupyter_notbook_config.py` file.\n",
    "\n",
    "    4) On the Azure portal create an inbound rule for port `port_number` (e.g. 9999 in above example), if not an inbound rule has been created for this port before. (To create an inbound rule: at the VM's overview page on Azure portl, click `Networking` and check if section `INBOUND PORT RULES` already has 9999 shown in PORT column. If not, click `Add inbound port rule`, then set `Destination port ranges` to be 9999, and modify `Name` to be \"Port_9999\". Finally click `Add`).\n",
    "    \n",
    "    5) On your local machine, open a web-browser and go to `http(s)://<VM IP address>:port_number/` as indicated in the above command output. When prompted for password, enter the password you have previously created. If you don't remember the password, finish steps 7) - 10). \n",
    "    \n",
    "    6) If you succesfully access `http(s)://<VM IP address>:port_number/` (ignore `Certificate error`), skip remaining steps. Otherwise, continue on below steps. \n",
    "    \n",
    "    7) In the concole, shut down the Jupyter server by typing `Ctrl+C`.\n",
    "    \n",
    "    8) Generate a jupyter configuration file.\n",
    "        $ jupyter notebook --generate-config \n",
    "    \n",
    "    9) Generate a jupyter password (create a password and enter it twice).\n",
    "        $ jupyter notebook password \n",
    "  \n",
    "    10) Start the jupyter notebook from the command line and go to step 5). \n",
    "        $ jupyter notebook --no-browser --ip=* \n",
    "\n",
    "By now, you should have the Jupyter server on and you can access the notebooks from the local copy of repo. Open the notebook `OperationalizeDLModelswithAMLCLI.ipynb` and make sure the kernel is set to be `Python [conda env: py35]`(To change a kernel, choose `kernel` - `Change kernel` in notebook menu).\n",
    "\n",
    "### Check configuration from Jupyter notebooks\n",
    "\n",
    "As a prerequisite, Azure CLI must be configured to make a set of \"az ml\" commands work for [model management](https://docs.microsoft.com/en-us/azure/machine-learning/preview/model-management-cli-reference). In a notebook cell, type `az -h` and `az ml -h`. The output should look similar as below screenshots. This check ensures relevant commands works in the [Deploy model as a Web Service](#deploy_model) section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries \n",
    "In this section, we will show you how to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "os.environ['KERAS_BACKEND'] = \"tensorflow\"\n",
    "import keras as K\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout\n",
    "from common.params import *\n",
    "from common.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force one-gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Improvement\n",
    "# 1. Make sure channels-first (not last)\n",
    "K.backend.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.4 |Anaconda custom (64-bit)| (default, Nov 20 2017, 18:44:38) \n",
      "[GCC 7.2.0]\n",
      "Keras:  2.1.4\n",
      "Numpy:  1.14.1\n",
      "Tensorflow:  1.6.0\n",
      "tensorflow\n",
      "channels_first\n",
      "GPU:  ['Tesla K80']\n",
      "CUDA Version 9.0.176\n",
      "CuDNN Version  7.0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"Keras: \", K.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"Tensorflow: \", tensorflow.__version__)\n",
    "print(K.backend.backend())\n",
    "print(K.backend.image_data_format())\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(get_cuda_version())\n",
    "print(\"CuDNN Version \", get_cudnn_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_symbol(n_classes=N_CLASSES):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), padding='same', activation='relu',\n",
    "                     input_shape=(3, 32, 32)))\n",
    "    model.add(Conv2D(50, kernel_size=(3, 3), padding='same', activation='relu'))    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(100, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(100, kernel_size=(3, 3), padding='same', activation='relu'))    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(m, lr=LR, momentum=MOMENTUM):\n",
    "    m.compile(\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        optimizer = K.optimizers.SGD(lr, momentum),\n",
    "        metrics = ['accuracy'])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing train set...\n",
      "Data does not exist. Downloading https://ikpublictutorial.blob.core.windows.net/deeplearningframeworks/cifar-10-python.tar.gz\n",
      "Extracting files...\n",
      "Preparing train set...\n",
      "Preparing test set...\n",
      "(50000, 3, 32, 32) (10000, 3, 32, 32) (50000, 10) (10000, 10)\n",
      "float32 float32 int32 int32\n",
      "CPU times: user 2.73 s, sys: 1.12 s, total: 3.85 s\n",
      "Wall time: 8.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Data into format for library\n",
    "x_train, x_test, y_train, y_test = cifar_for_library(channel_first=True, one_hot=True)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print(x_train.dtype, x_test.dtype, y_train.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 437 ms, sys: 2.63 s, total: 3.07 s\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load symbol\n",
    "sym = create_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.5 ms, sys: 359 µs, total: 28.9 ms\n",
      "Wall time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialise model\n",
    "model = init_model(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 50, 32, 32)        1400      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 32, 32)        22550     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 100, 16, 16)       45100     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 100, 16, 16)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 100, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3277312   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 3,441,592\n",
      "Trainable params: 3,441,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 21s 420us/step - loss: 1.8471 - acc: 0.3300\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 1.4364 - acc: 0.4761\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 1.2250 - acc: 0.5598\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 1.0637 - acc: 0.6220\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.9402 - acc: 0.6680\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.8518 - acc: 0.6993\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.7687 - acc: 0.7296\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 20s 396us/step - loss: 0.7046 - acc: 0.7530\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.6471 - acc: 0.7700\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.5980 - acc: 0.7868\n",
      "CPU times: user 2min 34s, sys: 31.2 s, total: 3min 6s\n",
      "Wall time: 3min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9de5867c88>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Main training loop: 1m16s\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=BATCHSIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 677 ms, sys: 50.6 ms, total: 728 ms\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Main evaluation loop\n",
    "y_guess = model.predict(x_test, batch_size=BATCHSIZE)\n",
    "y_guess = np.argmax(y_guess, axis=-1)\n",
    "y_truth = np.argmax(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7639\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", 1.*sum(y_guess == y_truth)/len(y_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mylogin/notebooks/DeepLearningModelDeployment\n"
     ]
    }
   ],
   "source": [
    "#get the current working directory\n",
    "print(os.getcwd()) \n",
    "\n",
    "#list files in current working directory\n",
    "# os.listdir(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = os.getcwd()\n",
    "o16n_path = os.path.join(local_path,'o16n')  \n",
    "model_path = os.path.join(o16n_path,'kerastfmodel')\n",
    "model_file_name = os.path.join(model_path,'kerastfmodel.h5')\n",
    "score_file_name = os.path.join(model_path, 'score.py')\n",
    "\n",
    "\n",
    "if not os.path.exists(local_path):\n",
    "    os.makedirs(local_path)\n",
    "if not os.path.exists(o16n_path):\n",
    "    os.makedirs(o16n_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mylogin/notebooks/DeepLearningModelDeployment/o16n/kerastfmodel\n",
      "/home/mylogin/notebooks/DeepLearningModelDeployment/o16n/kerastfmodel/score.py\n"
     ]
    }
   ],
   "source": [
    "print(model_path)\n",
    "print(score_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model.save(model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a scoring script \n",
    "In order to create a web service, you will create a scoring script that will load the models, perform the prediction, and return the result. \n",
    "Azure ML uses init() and run() functions inside this scoring script for that purpose:\n",
    "\n",
    "   - The init() function initializes the web service and loads the saved model. \n",
    "   - The run() function uses the model and the input data to return a prediction which is executed on a scoring call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/mylogin/notebooks/DeepLearningModelDeployment/o16n/kerastfmodel/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $score_file_name\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import keras as K\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageOps\n",
    "import base64\n",
    "import json\n",
    "\n",
    "def init():\n",
    "    \n",
    "    global model  \n",
    "\n",
    "    print(\"Executing init() method...\")\n",
    "    print(\"Python version: \" + str(sys.version) + \", keras version: \" + K.__version__)\n",
    "    # Load the model \n",
    "    model = K.models.load_model('kerastfmodel.h5')\n",
    "    return\n",
    "\n",
    "\n",
    "def run(inputString):\n",
    "    \n",
    "    responses = []\n",
    "    base64Dict = json.loads(inputString)\n",
    "\n",
    "    for k, v in base64Dict.items():\n",
    "        img_file_name, base64Img = k, v\n",
    "    decoded_img = base64.b64decode(base64Img)\n",
    "    img_buffer = BytesIO(decoded_img)\n",
    "    imageData = Image.open(img_buffer).convert(\"RGB\")\n",
    "\n",
    "    # Evaluate the model using the input data\n",
    "    img = ImageOps.fit(imageData, (32, 32), Image.ANTIALIAS)\n",
    "    img_conv = np.array(img) # shape: (32, 32, 3)\n",
    "    # Scale pixel intensity\n",
    "    x_test = img_conv / 255.0\n",
    "    # Reshape\n",
    "    x_test = np.moveaxis(x_test, -1, 0)\n",
    "    x_test = np.expand_dims(x_test, 0)  # shape (1, 3, 32, 32)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    # print(y_pred)\n",
    "    LABELS = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"fog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    resp = {img_file_name: str(LABELS[y_pred[0]])}\n",
    "\n",
    "    responses.append(resp)\n",
    "    return json.dumps(responses)\n",
    "    \n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    # input data\n",
    "    img_path = 'automobile8.png'\n",
    "    encoded = None\n",
    "    with open(img_path, 'rb') as file:\n",
    "      encoded = base64.b64encode(file.read())\n",
    "    img_dict = {img_path: encoded.decode('utf-8')}\n",
    "    body = json.dumps(img_dict)\n",
    "    resp = run(body)\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model deployment as web service\n",
    "\n",
    "In this section, we show the steps to deploy the previously trained model on [Azure Container Service (ACS)](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft.acs) via [Azure CLI](https://azure.microsoft.com/en-us/blog/announcing-general-availability-of-vm-storage-and-network-azure-cli-2-0/). \n",
    "\n",
    "During this section, following Azure resources will be created:\n",
    "    \n",
    "    - Resource group defined in variable YOUR_RESOURCE_GROUP\n",
    "        * Machine Learning Model Management\n",
    "        * cluster environment (Microsoft.MachineLearningCompute/operationalizationClusters)\n",
    "    - Resource group created during the cluster environment provision (YOUR_RESOURCE_GROUP plus\"-azureml-xxxxx\") \n",
    "        * Container registry\n",
    "        * Container service\n",
    "        * .... a bunch of other automatically provisoned resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mylogin/notebooks/DeepLearningModelDeployment/o16n/kerastfmodel/conda_dependencies.yml'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy conda_dependencies.yml file into the directory model_path. This file is needed for model deployment.\n",
    "import shutil\n",
    "yml_file_name = os.path.join(local_path,'common/conda_dependencies.yml')\n",
    "shutil.copyfile(yml_file_name, os.path.join(model_path, os.path.split(yml_file_name)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conda_dependencies.yml', 'score.py', 'kerastfmodel.h5', 'automobile8.png']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list files in current working directory\n",
    "os.listdir(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login your Azure account and set Azure subscription\n",
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing `!az login`, you will see following output message: To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code xxxxxxx to authenticate. Finish this login task in a browser and close the browser page.\n",
    "\n",
    "In the output of this command, you should see following information about your Azure subscription. Locate the \"name\" field and use it as the SUB_Name in the next cell. If you have more than one Azure subscritons, please choose one for this execercise. \n",
    "\n",
    "      {\n",
    "        \"cloudName\": \"AzureCloud\",\n",
    "        \"id\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n",
    "        \"isDefault\": true,\n",
    "        \"name\": \"Your Subscription Name\",\n",
    "        \"state\": \"Enabled\",\n",
    "        \"tenantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n",
    "        \"user\": {\n",
    "          \"name\": \"yourloginEmail@xxx.com\",\n",
    "          \"type\": \"user\"\n",
    "        }\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Variable Names for Deployment\n",
    "SUB_Name = '\"Your Azure Subscription name\"' # Make sure you include double quote for your Azure subscription name\n",
    "LOCATION = 'Pick an Azure Region Here!! e.g. eastus2' # resource location\n",
    "DEPLOY_NAME = 'Pick a Deployment Name Here!!' # limite DEPLOY_NAME to 14 characters or less\n",
    "YOUR_RESOURCE_GROUP = DEPLOY_NAME + 'rg' \n",
    "MM_ACCOUNT = DEPLOY_NAME + 'mmacc'     \n",
    "CLUSTER_SERVICE_NAME = DEPLOY_NAME + 'clus' + 'srvc'\n",
    "CLUSTER_ENV_NAME = DEPLOY_NAME + 'clus' + 'env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account set -s $SUB_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a resource group (optinal, only if you are about to create a new resource group)\n",
    "!az group create --l $LOCATION --name $YOUR_RESOURCE_GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute following command to create a Machine Learning Model Management resource in the resource group\n",
    "!az ml account modelmanagement create -l $LOCATION -n $MM_ACCOUNT -g $YOUR_RESOURCE_GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute fllowing command to create the cluster environment (Microsoft.MachineLearningCompute/operationalizationClusters)\n",
    "# !az ml env setup --cluster -l $LOCATION -n $CLUSTER_ENV_NAME -g $YOUR_RESOURCE_GROUP\n",
    "print('az ml env setup --cluster -l ', LOCATION, '-n ', CLUSTER_ENV_NAME,'-g ', YOUR_RESOURCE_GROUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the above command in a new CLI console. (You need to ssh into the VM from your local machine again, as the Jupyter server is still running from previous openned console.) At the prompt, type 'n' for question \"Reuse storage and ACR (Y/n)?\" and 'Y' for question \"Continue with this subscription (Y/n)?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check the information on the environment with the following command. \n",
    "!az ml env show -g $YOUR_RESOURCE_GROUP -n $CLUSTER_ENV_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml account modelmanagement set -n $MM_ACCOUNT -g $YOUR_RESOURCE_GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az ml env set -g $YOUR_RESOURCE_GROUP -n $CLUSTER_ENV_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conda_dependencies.yml', 'score.py', 'kerastfmodel.h5', 'automobile8.png']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the current working directory to model_path\n",
    "# os.chdir(model_path)\n",
    "# list files in current working directory\n",
    "# Make sure current directory contains 'score.py', 'kerastfmodel.h5', and 'conda_dependencies.yml'\n",
    "\n",
    "os.listdir(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create web service - this step can take several minutes\n",
    "!az ml service create realtime --model-file kerastfmodel.h5 -f score.py -n $CLUSTER_SERVICE_NAME -r python -c conda_dependencies.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the web service is succesfully deployed, you should see a field \"Service ID\" from the output information. \n",
    "Take this information and put in CLUSTER_SERVICE_ID in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_SERVICE_ID = 'Your cluster service Id here!!'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
